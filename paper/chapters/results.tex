\chapter{Results}
\label{chap:results}

The following sections briefly describe the experimental setup, before showing the results for each experiment discussed in Chapter \ref{chap:aims}.
These experimental results are also discussed in this chapter.


\section{Hyperparameter gridsearch}

As a first experiment, the de novo SMILES MassSpecGym model was reproduced and extended with a larger gridsearch.
The extended hyperparameter search grid for the retrained SMILES models, along with MassSpecGym's grid search results, is presented in Table \ref{tab:gridsearch}.
The hyperparameter combination that achieved the lowest validation loss is highlighted in bold.

\begin{table}[h]
	\caption{
		Gridsearch MassSpecGym vs Gridsearch Thesis for the de novo SMILES transformer (lowest validation loss models in bold).
	}
    \resizebox{\textwidth}{!}{
	\begin{tabular}{p{6cm}W{c}{4cm}W{c}{4cm}}
		\toprule
                \textbf{Hyperparams} & \textbf{MassSpecGym} & \textbf{Thesis Gridsearch} \\
            \midrule
                Learning Rate & $\mathbf{3\cdot 10^{-4}}, 1\cdot 10^{-4}, 5\cdot 10^{-5}$ & $1\cdot 10^{-3}, 3\cdot 10^{-4}, \mathbf{1\cdot 10^{-4}}$\\
                Batch Size & $512, \mathbf{1024}$ & $\mathbf{512}, 1024, 2048$ \\
                $n$ predictions & $\mathbf{10}$ & $\mathbf{10}$ \\
                Transformer hidden dimensionality & $\mathbf{256}, 512$ & $\mathbf{128}, 256, 512$ \\
                Number of attention heads & $\mathbf{4}, 8$ & $2, 4, \mathbf{8}$ \\
                Number of encoding layers & $\mathbf{3}, 6$ & $\mathbf{2}, 3, 4$ \\
                Number of decoding layers & $\mathbf{4}$ & $2, 3, \mathbf{4}$ \\
		\midrule
	\end{tabular}}
	\label{tab:gridsearch}
\end{table}

By retraining with the extended search grid, several models were found that outperformed the retrained model with the best hyperparameter combination from MassSpecGym, according to the validation loss.
Note that the number of predictions (i.e. top-1 or top-10) is kept constant as this does not influence the loss.
There is a substantial difference between the optimal hyperparameters from the retrained gridsearch and MassSpecGym's results.
Because the loss function does not accurately describe the model's ability to predict de novo molecules from \ac{MS/MS},
a naive sampler, that just iteratively samples from the model's output distribution (as described in Section \ref{sec:stochastic_samplers}), was used to predict SMILES on the validation set.
The evaluation of the SMILES predictions from both models using this sampler can be found in Figure \ref{fig:gridsearch_vs_paper}.
The \ac{MCES} distance on the SMILES and Tanimoto similarity on the converted fingerprints are calculated in the strict (top-1) and more relaxed (top-10) evaluation settings.
The top-1 and top-10 exact match accuracy for both models is (close to) zero and was therefore excluded from Figure \ref{fig:gridsearch_vs_paper}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results/gridsearch_vs_paper.png}
    \caption{Performance comparison between the retrained SMILES model from the MassSpecGym paper versus the lowest validation loss SMILES model from the gridsearch, evaluated on the validation set.}
    \label{fig:gridsearch_vs_paper}
\end{figure}

It is clear that the retrained SMILES model from MassSpecGym outperforms the model with the lowest validation loss on every metric.
This shows that there is a discrepancy between the loss function and the evaluation metrics as lower validation loss does not necessarily indicate better predictions.
A big contributing factor to this performance difference is the fact that the retrained MassSpecGym model is able to predict 10\% more valid SMILES, as invalid SMILES have the maximum MCES distance and zero Tanimoto similarity.
The validity of the SMILES prediction could rely on the sampler and the temperature scaling of the output distribution. This is tested in the next experiment.


\section{Samplers benchmark}

To test how much temperature scaling and different samplers can influence model performance, different samplers were used to predict the validation set of the model with the lowest loss from Table \ref{tab:gridsearch}.
For this experiment, the Tanimoto similarity was directly correlated to the MCES distance and will thus not be shown in the plots.
All performance differences shown with the MCES distance always show the same relative Tanimoto similarities.

Firstly, the influence of temperature scaling is tested using the naive sampler for the top-1 and top-10 settings.
These results are shown in in Figure \ref{fig:naive_and_greedy}.
Because this temperature difference will influence the randomness of the sampler's predictions, a simple greedy sampler that always predicts the most likely token is shown as a deterministic baseline.
This greedy sampler will always predict the same SMILES and can thus only be evaluated in the top-1 setting.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/results/samplers/naive_and_greedy.png}
    \caption{Evaluation of the naive sampler with different temperatures on the validation set.}
    \label{fig:naive_and_greedy}
\end{figure}

From Figure \ref{fig:naive_and_greedy}, it is clear that the naive sampler performs poorly when only one prediction is evaluated (top-1).
Increasing the randomness (by increasing the temperature) for the top-1 naive sampler, only worsens the MCES distance.
As expected, the top-1 naive sampler's MCES distance converges to that of the greedy sampler as the temperature approaches zero, since extremely low temperatures force the model to only predict the most likely token.

The naive sampler benefits greatly from its randomness with top-10 evaluation, by performing better than the greedy sampler according to the MCES distance.
It shows to have an optimal temperature around $0.7$, for this SMILES model.
A higher temperature increases the randomness of the sampler too much, causing the amount of valid predictions to drop.
A lower temperature starts to force the predictions towards the greedy sampler.
These results shows that, with the temperature parameter, there is a tradeoff to be made between exploitation (low temperature) and exploration (high temperature).
The optimal temperature will depend on which evaluation setting is used.
In top-1 evaluation, each invalid prediction is penalized; therefore, exploitation — which maximizes the number of valid predictions — is important.
With top-10 evaluation, if at least one of the ten predictions per spectrum is valid, the other invalid predictions are not penalized. This gives the sampler room for exploration.
This explains the results in Figure \ref{fig:naive_and_greedy}.
The greedy sampler maximizes exploitation and thus outperforms the naive sampler in the top-1 evaluation setting.
In contrast to the naive sampler, the greedy sampler is incapable of exploring multiple predictions.
Figure \ref{fig:naive_and_greedy} shows that exploration does result in better predictions, but less valid predictions, allowing the naive sampler to perform better with top-10 evaluation.

\subsection{Stochastic Samplers}

The randomness of the naive sampler can be tweaked using samplers such as top-k and top-p (nucleus sampling) as described in Section \ref{sec:samplingmethods}.
Because the stochastic samplers only benefit from their randomness when evaluating in a top-10 setting, only these results will be shown on the plots.
The temperature has shown to influence performance, therefore, a temperature search was conducted for each sampler with different parameters $k$ or $p$, where the optimal temperature (according to the lowest MCES distance) was selected.
This ensures that only the best results are shown for each the sampler with its parameter.
For completeness, the top-1, along with the temperature search results can be found in the Appendix Section \ref{sec:sampler_full_results}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/results/samplers/top-k.png}
    \caption{Evaluation of the (top-10, temperature optimized) top-k sampler and naive sampler on the validation set.}
    \label{fig:top-k}
\end{figure}

Figure \ref{fig:top-k} shows the performance of the top-k sampler with ranging values for $k$, top-10 evaluated on the validation set.
This figure shows that top-k sampling can slightly improve performance compared to the naive sampler when $k > 5$.
When the sampler gets limited too much, with a low $k$, it performs slightly worse. 
Note that the y-axis is scaled for visibility, the improvement is very small.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/results/samplers/top-p.png}
    \caption{Evaluation of the (top-10, temperature optimized) top-p sampler and naive sampler on the validation set.}
    \label{fig:top-p}
\end{figure}

Figure \ref{fig:top-p} shows the performance of the top-p (nucleus) sampler for different values of $p$, top-10 evaluated on the validation set.
With top-k, limiting the randomization seems to slightly improve performance of the sampler, Figure \ref{fig:top-p} shows that this does not hold for the top-p sampler.
Overall, these randomization limiting samplers do not considerably improve the naive sampler's performance.

\subsection{Deterministic Samplers}

The beam search sampler is a deterministic sampler that improves upon the greedy sampler by increasing its search space to find multiple predictions as explained in Section \ref{sec:samplingmethods}.
Figure \ref{fig:beam-search} shows its performance for different search space sizes (beam widths).
The length regularization parameter $\alpha$ from the scoring function was also benchmarked for different beam widths but showed to have no influence on performance.
Only when large values were used ($\alpha > 10$), performance tanked to very poor results.
Forcing the sampler to predict longer token sequences with parameter $\alpha$ thus only hinders performance.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/results/samplers/beam_search.png}
    \caption{Evaluation of the beam search sampler compared to the greedy and naive sampler on the validation set.}
    \label{fig:beam-search}
\end{figure}

Figure \ref{fig:beam-search} shows that the top-1 and top-10 MCES distance for the beam search sampler improves with increased search space as expected.
The spike in MCES distance and fraction of valid predictions on the first data point is because the beam width is 1, meaning it is essentially the same as the greedy sampler.
There is a clear performance difference between the top-1 and top-10 evaluation settings, indicating that the prediction with the lowest score is often not the prediction with the lowest MCES distance.
This shows that the scoring function, and thus the output probabilities from the model, are not optimal for extracting the correct molecular structure.
This again indicates a flaw with the loss, for which the model was optimized.

These suboptimal output probabilities also explain why the beam search sampler, which is inherently better than the greedy sampler at exploiting the model's output, performs worse in the top-1 setting than the greedy sampler.
This exploitation is clearly visible on the fraction of valid predictions plot in Figure \ref{fig:beam-search}, where the best scoring prediction from the sampler (with a beam width > 1) is always a valid molecule.

The top-10 MCES distance from the beam search sampler is still notably worse than the best top-10 naive sampler results, showing that for top-10 evaluation exploration is still preferred over exploitation.

Overall, from these results, the greedy sampler seems to perform best for the top-1 evaluation setting, while the naive sampler excels in the top-10 evaluation setting.
For all the following experiments in this thesis, the greedy and naive samplers are only used for the top-1 and top-10 evaluation setting respectively.

\section{\ac{BPE} as pretraining}

The previous experiment used a SMILES de novo model with a vocabulary, precomputed using \ac{BPE} on the unlabeled dataset with 4,000,000 SMILES from MassSpecGym.
To measure the influence of the \acf{BPE} on the model's performance, in this experiment, SMILES models were trained with different tokenizers.
These tokenizers only differ in vocabulary and, consequently, vocabulary size.
The training hyperparameters are kept the same for all models for consistency.

A simple tokenizer that does not pre-compute any substrings with \ac{BPE} in its vocabulary was used to train a baseline model.
The other tokenizers used MassSpecGym's unlabeled SMILES datasets of different sizes.
These datasets are further referred to as 4M and 118M, for the datasets with 4,000,000 and 118,000,000 SMILES respectively.
Using these unlabeled datasets can be seen as a form of pretraining, where frequently occurring substrings are captured as single tokens in the vocabulary.
All tokenizers, pre-computed with \ac{BPE}, also use the training SMILES in its \ac{BPE} corpus.
One tokenizer only used the SMILES from the training set to compute the \ac{BPE} patterns.
This model is a \ac{BPE} baseline to see how using the unlabeled datasets can influence performance.

For the 4M and 118M dataset, a second tokenizer was pre-computed where the \ac{BPE} algorithm was given a more strict cut-off for grouping characters, by increasing the minimal frequency parameter from 2 (the default value) to 10.
Lastly, because the vocabulary size of the 118M tokenizer reached the algorithm's threshold, a tokenizer was computed on the 118M dataset where computation halted as soon as the vocabulary reached the size of 5200 (equal to the vocabulary size of the 4M tokenizer). 

Figure \ref{fig:bpe} shows the evaluation of the models trained with different \ac{BPE} tokenizers, using the greedy sampler and the best naive sampler for the top-1 and top-10 evaluation settings respectively.
A new evaluation metric, the fraction of novel predictions, can be seen in the last plot.
It shows the fraction of valid predictions that are not identical to a molecule in the training set.
This measures how much the model is able to predict unseen SMILES as a metric to quantify overfitting.
The complete temperature search for the naive top-10 sampler can be found in the Appendix Figure \ref{fig:bpe_appendix}.

The most notable result from Figure \ref{fig:bpe} is that models trained with a \ac{BPE} tokenizer have more valid predictions.
Using \ac{BPE} does indeed succeed in reducing invalid predictions, achieving its intended purpose.
A huge drawback however, can be seen when looking at the fraction of novel predictions.
Using \ac{BPE} with default parameters nearly halves the number of novel predictions compared to the model trained with a non-BPE tokenizer, which indicates that using \ac{BPE} increases overfitting.

The \ac{BPE} 4M tokenizer is used in MassSpecGym's de novo SMILES model.
Figure \ref{fig:bpe} shows that it has the best performance looking at the MCES distance and Tanimoto similarity, it also achieves one of the highest number of valid predictions compared to the other tokenizers.
The poor amount of novel predictions does question if this result is due to overfitting, as more than half of its predictions are identical to molecules from the training set.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.79\textwidth]{figures/results/bpe_with_tanimoto.png}
    \caption{Evaluation on the validation set of SMILES models trained with different Byte-Pair encoded tokenizers.
    Note that the y-axis of the fraction of valid predictions plot does not start at zero for readability.}
    \label{fig:bpe}
\end{figure}

A strange result is that the use of the larger 118M dataset does not improve performance compared to the 4M dataset when comparing MCES distance and Tanimoto similarity.
More data should result in better generalization.
This can however be due to the fact that the quality of this dataset is considerably worse.
As explained in the MassSpecGym paper, the 118M dataset consists of all the available data from PubChem. 
No quality control was conducted for this dataset, which allows for an unrealistic distribution of chemical classes in the dataset.
An overrepresentation of one chemical class could severely influence the patterns found by the \ac{BPE} algorithm.
In contrast, the 4M dataset does have a controlled representation of multiple chemical classes.
This shows that quality is preferred over quantity.

Limiting the vocabulary size also shows to negatively impact performance, although for the 118M tokenizer it does increase the number of valid predictions.
Increasing the minimal frequency for patterns to be grouped also shows to noticeably increase the number of novel predictions.
This means that the overfitting through \ac{BPE} can be limited.
The 118M tokenizer with a minimal frequency of 10 almost reaches the same number of novel predictions as the model without \ac{BPE}, with almost $10\%$ more valid predictions for the top-1 greedy sampler.
Sadly, its MCES distance and Tanimoto similarity is the worst out of all the models from this experiment.

Overall, using \ac{BPE} increases the number of valid predictions at the cost of novel predictions.
When solely looking at the MCES distance and Tanimoto similarity, using \ac{BPE} can (in some cases) improve the model's performance.
The main advantage is that by grouping multiple tokens as one, \ac{BPE} decreases the number of tokens the model has to predict.
When the model has less tokens to predict, it will make less mistakes, leading to more valid molecules.
The number of novel predictions does put these results in doubt, questioning the generalization of these models.
The model will favour grouped tokens seen in the training data, and by having less tokens to predict, it will more often lead to predictions that are identical to molecules from the training set.
When using \ac{BPE}, the parameters of the algorithm should be tuned such that a balance between performance and minimal overfitting could be found.

Because the \ac{BPE} 4M tokenizer is the standard used by MassSpecGym's de novo SMILES model and reached the best MCES distance, this tokenizer is used for the following experiments in thesis. 

\section{Augmentation}

The influence of different data augmentation methods was measured by retraining MassSpecGym's SMILES de novo model with different augmented datasets.
For each of these models, along with the baseline model without augmentation, the MCES distance, Tanimoto similarity, fraction of valid predictions and fraction of novel predictions were evaluated on the validation set.
As before, a greedy sampler was used for the top-1 setting, while a temperature search was conducted with the naive sampler for the top-10 setting.
Full results for the naive top-10 temperature search can be found in the appendix section \ref{sec:temp_search_appendix}.

\subsection{SMILES augmentation}

Labeled spectra can be augmented by calculating synonyms of the SMILES label that represent the same molecular graphs.
The dataset can then be duplicated by copying the spectra and using a SMILES synonym as label.
This should allow the model to be more robust against repeated SMILES in the training set.
For this experiment three models were trained, for which the whole training set was duplicated once, twice and five times with SMILES synonyms (referred as 1x synonym, 2x synonyms and 5x synonyms respectively in the results).
For all these models the original training data with the original SMILES was kept in the training set.
Figure \ref{fig:smiles_augm} shows the evaluated results of these models using different evaluation metrics and settings.
The full naive sampler's top-10 temperature search results can be found in the appendix figure \ref{fig:smiles_augmentation_appendix}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.79\textwidth]{figures/results/smiles_augmentation_with_tanimoto.png}
    \caption{Evaluation on the validation set of SMILES models trained with different SMILES augmented datasets}
    \label{fig:smiles_augm}
\end{figure}

From figure \ref{fig:smiles_augm} it is clear that SMILES augmentation does not seem to improve the de novo SMILES model from MassSpecGym.
The MCES distance, Tanimoto similarity and number of valid predictions get worse when more augmented SMILES are used for training.
Only the number of novel molecules, top-1 predicted, seems to increase by using some augmentation.
Too much augmentation reduces this again.

Overall, MassSpecGym's de novo SMILES model does not seem to benefit from any SMILES augmented data.

\subsection{Spectral augmentation}

Input spectra can be augmented by simulating variations in precursor masses through spectral shifts.
Labeled spectra were augmented by applying the same random mz-shift to the peaks of a spectrum and copying its SMILES label.
Two models were trained for this experiments, with $20\%$ and $100\%$ of the training spectra augmented (referred as $20\%$ mz shift augm. and $100\%$ mz shift augm. respectively in the results).
Again, the original training data is kept in the training set.
Figure \ref{fig:spectral_augm} shows the evaluated results of these models using different metrics and evaluation settings.
As usual, the full naive samplers top-10 temperature search results can be found in the appendix figure \ref{fig:spectral_augmentation_appendix}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/results/spectrum_augmentation_with_tanimoto.png}
    \caption{Evaluation on the validation set of SMILES models trained with different spectral augmented datasets}
    \label{fig:spectral_augm}
\end{figure}

In contrast to the previous augmentation method, spectral augmentation does seem to help the de novo SMILES model from MassSpecGym.
While there are no significant improvements looking at the MCES distance and Tanimoto similarity, the $20\%$ augmented model is able to predict more valid and especially more novel molecules.
Too much augmentation, again, hinders performance as the original data gets drowned in augmented data.

This method of augmentation, when moderately used, clearly seems to help the model be less prone to overfitting, as the top-1 greedy sampler is able to predict more than $20\%$ more novel molecules. 

\section{Molecular representations benchmark}

Different molecular representations can be used to predict the structure of a molecule.
This experiment benchmarks the string-based molecular representations for the de novo structure prediction from \ac{MS/MS} task.
InchIkey is the only representation discussed in section \ref{sec:molrepr} that is not used to train a model, as predicting hash keys is not possible.
All models were trained with the same hyperparameters.
Because tokenizers with a byte-pair encoded vocabulary have shown to influence performance, for each molecular representation two models were trained.
One model used a byte-pair encoded tokenizer, the other used a basic character tokenizer.
For all these models, a greedy sampler was used to evaluate the top-1 setting, while a temperature search was performed for the naive sampler with top-10 evaluation.
Each molecular representation was thus evaluated four times.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/results/representations_with_tanimoto.png}
    \caption{Evaluation on the validation set of models trained with different molecular representations}
    \label{fig:representations}
\end{figure}

The full results for the naive top-10 evaluation with the different temperatures can be found in the appendix figure \ref{fig:representations_appendix}.
Figure \ref{fig:representations} shows the MCES distance, Tanimoto similarity, fraction of valid predictions and fraction of novel predictions of the predictions from models trained on different molecular representations.

For the top-1 evaluation setting, SMILES outperforms the other molecular representations according to the MCES distance and Tanimoto similarity seen in figure \ref{fig:representations}.
SMILES and SELFIES seem to perform the best for the top-10 evaluation according to their MCES distances, the Tanimoto similarity does not reflect this however, with the SMILES models outperforming the SELFIES models.

The models trained with SELFIES predicted the most valid predictions, as this is an inherent property of SELFIES, but noticeably also predicted the most novel molecules.
This indicates that the SELFIES models are less prone to overfitting on the training labels.

While DeepSMILES is closely related to SMILES with only slight modifications to reduce the chance of invalid predictions through syntax errors, it noticeably performs worse than SMILES, even having noticeably less valid predictions with top-10 evaluation.
The models trained using DeepSMILES are thus more prone to chemical errors as (almost) no syntax errors are possible.
An example of a prediction with a chemical error made by a DeepSMILES model can be seen in figure \ref{fig:invalid_pred}, where five bonds on one carbon atom are present, which is not possible in a stable environment.
The improvements DeepSMILES introduces over SMILES to combat invalid predictions for autoregressive generation seem to have the opposite results for de novo structure prediction, by introducing more chemical errors.
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/results/invalid_pred.JPG}
    \caption{2-dimensional representation of prediction with chemical error (SMILES: C1=CC=C(C(=C1C2=CC(=O)(C3=C(O2)C=CC(=C3O)O)O)O)O )}
    \label{fig:invalid_pred}
\end{figure}

Out of the four representations benchmarked, InchI clearly shows to be the least fit for autoregressive generation, with most of its predictions being invalid.
This is caused by the fact that InchI has a strict layered structure to ensure its desired properties, which the models struggle to predict correctly.
When the model does succeed to predict an InchI with a valid syntax, it almost always was present in the training set, showing heavy signs of overfitting.
To help the model understand the different layers of the InchI representation, a model was trained with a different decoder for each relevant InchI layer. 
The results for this model can be found in the appendix figure \ref{fig:layered_inchi}.
This model was only able to predict a few valid predictions and struggled to synchronize the outputs of the different decoders.
The poor performance of InchI for this task is expected as the molecular representation was not designed for structure prediction.

The same conclusions from the \ac{BPE} experiment can be drawn for this experiment, where the use of a pre-computed vocabulary using \ac{BPE} improves the MCES distance and number of valid predictions but makes the model more prone to overfitting by having less novel predictions.
Only for the SELFIES top-10 evaluation, using \ac{BPE} also improves the number of novel predictions.

Overall, in this experiment, SMILES shows to be the best molecular representation when a top-1 evaluation is used.
Taking the amount of novel predictions into account, SELFIES could also be considered for top-10 evaluation.
