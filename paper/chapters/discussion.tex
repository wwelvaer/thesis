\chapter{Discussion}
\label{chap:discussion}

This chapter first discusses certain problems with the experimental setup that could impact performance, before discussing certain flaws and improvements for the experiments.
The last section will briefly mention recommendations for the future of de novo structure prediction from \ac{MS/MS}.

\section{Biases and overfitting}
\label{sec:overfitting}
 
A recurring problem for string-based autoregressive models is the number of predictions that are identical to a label found in the training set.
When this occurs, it shows that a model is not able to achieve the task of predicting a de novo molecular representation from \ac{MS/MS}, but only repeats the data it has been trained with.
There are a few contributing factors to this overfitting problem with the setup used in this thesis.
Along with the problem of overfitting, there are some biases present in the experimental setup that are discussed in the following sections. 

\subsection{Duplicate training labels}
\label{sec:duplicate_training_labels}
As mentioned in the paper from MassSpecGym \cite{bushuiev2024massspecgym}, the labeled dataset is a collection of the most qualitative annotated, open-source mass spectrometry datasets.
Even though a lot of filtering was performed after thorough quality control, the amount of duplicate molecules is significant, with 2.0\% of the unique molecules covering 25.1\% of the training data.
Figure \ref{fig:duplicate_smiles} shows the amount of SMILES that have multiple occurrences in the training set.
This imbalance does not necessarily reflect the real-world imbalance of molecules as it is caused by the use of domain-focused datasets.
This causes the models to be trained with a bias towards these more frequently occurring molecules in the training set.
This imbalance is also present in the validation (and test) set, where the most frequently occurring molecule accounts for 2.7\% (out of the almost 20.000 entries) of the validation data.
Because of this imbalance, the models were evaluated with a bias towards these more abundant molecules.

\subsection{Exposure bias}

During training, a teacher forcing approach is used, where only the ground truth tokens are used to generate the next token iteratively.
At inference time, when the model is sampled, the model does not get this help and has to rely on its own previous (incorrect) predictions.
This is often referred to as exposure bias \cite{schmidt2019generalization}.
The previously discussed imbalance of the training set only makes this bias worse, by training on these frequently occurring token combinations.
This severely hinders the generalization of the model.
An example of this exposure bias can be clearly seen in the results from the hyperparameter gridsearch (Figure \ref{fig:gridsearch_vs_paper}), where a model with a lower validation loss performed worse after sampling.
The loss has this teacher forced advantage, while the metrics calculated on the sampled predictions do not.
Because all models trained in this thesis are optimized for validation loss, it can be questioned if they have been overfit on these teacher forced training molecules.

\subsection{Evaluation metrics}
The two metrics that are used to evaluate a molecular prediction, Tanimoto similarity and \ac{MCES} distance, are somewhat flawed.
As explained in section \ref{sec:tan_sim}, the Tanimoto similarity uses molecular fingerprints that are unable to store all structural information.
When the predicted structure is converted to a molecular fingerprint, some structural information is lost.
The prediction will thus not be evaluated completely.
The \ac{MCES} distance on the other hand does evaluate the whole molecular structure, but can in some cases behave sub-optimally.
Looking at the formula in Section \ref{sec:mces_dist}, when the evaluation molecules have few edges in their molecular graphs, a model could just minimize the edges in its prediction instead of finding the \acf{MCES}.
In an extreme case, where the model only predicts nothing (e.g. empty SMILES, zero edges in the molecular graph), the MCES distance will be equal to the number of edges of the ground-truth molecular graph.
Figure \ref{fig:edges} shows the distribution of edges in the molecular graphs from the molecules in MassSpecGym's labeled dataset.
With an average number of edges of 37.5, a model that predicts empty SMILES would have an evaluated MCES distance of 37.5 using MassSpecGym.
While during training, the model does not have access to the MCES distance and will thus probably not reach this extreme case, it does question if some \ac{MCES} distances have a bias towards shorter predictions by this flaw.

Another limitation of both metrics is their inability to distinguish between de novo molecules and those seen during training.
This can cause problems when the models are not powerful enough to outperform overfitted models.
A model that, instead of predicting de novo molecules, would just rank the training molecules and return the most similar ones (similar to frequently used database lookup methods), could appear relatively good at predicting de novo molecules when only the \ac{MCES} distance and Tanimoto similarity are considered.
Only the number of novel molecules accurately shows whether a model can generate de novo molecules.
Hence, to compare models, and decide which is better at predicting de novo molecules from \ac{MS/MS}, multiple metrics have to be considered.

\section{Experimental flaws and improvements}

In hindsight, some of the experiments conducted in this thesis could still be improved upon by taking the results from Chapter \ref{chap:results} and discussion points from Section \ref{sec:overfitting} into account.
This section discusses the flaws and possible improvements for the experiments from Chapter \ref{chap:results}.

\begin{description}
    \item[Choice baseline model] The SMILES de novo model with \ac{BPE} computed tokenizer from MassSpecGym was chosen as a baseline model for most of the experiments. 
In these experiments however, it shows that this model was severely overfit on the training SMILES, as almost half of its predictions were identical to SMILES from the training set.
The first two experiments (i.e. samplers and \ac{BPE} benchmark) should in an ideal case also have to be repeated for a model that is able to predict more de novo molecules, and thus is more robust against overfitting (e.g. SELFIES model).
For the augmentation experiment this would probably not drastically change this experiment's results, as augmentation has proven to be a great tool to combat overfitting \cite{shorten2019survey}.
    \item[SMILES augmentation] A flaw in the setup of the SMILES augmentation experiment, is that all augmented models used the same tokenizer, for which \ac{BPE} was only computed on the canonicalized SMILES from the training set.
Only patterns from canonicalized SMILES were thus grouped in the vocabulary.
Random SMILES therefore used more tokens to describe their sequence, making it harder for the model.
This experiment also may have used too much augmentation, compared to the spectral augmentation experiment.
Too much augmentation will drown out the original data.
A model that was trained with e.g. 20\% augmented data may have improved performance.
    \item[SMILES bias for molecular representations benchmark] The reason SELFIES and DeepSMILES do not outperform the SMILES model, while in theory they should be more robust, could be due to multiple reasons.
(1) The optimal \ac{BPE} SMILES hyperparameters were used to train all the models with different molecular representations.
This could affect the performance compared to the SMILES model, as these hyperparameters might not be optimal for the other models.
In an ideal case, a hyperparameter search would have to be conducted to accurately compare the best performance for each model.
(2) The samplers for each evaluation setting were chosen based on the results from the sampler benchmark, performed using a \ac{BPE} SMILES model.
Even though a temperature search was performed for the naive sampler,
the samplers used for each evaluation setting may not be optimal for models with different molecular representations.
The SMILES \ac{BPE} model, hence, had a noticeable advantage in this experiment. It is then only logical that it achieved the best performance.
    \item[Counteract dataset imbalance with augmentation]Instead of adding augmented data, the augmentation techniques described in Section \ref{sec:augmentation} could be used to counteract the imbalance of MassSpecGym's labeled dataset discussed in Section \ref{sec:duplicate_training_labels}.

    For SMILES augmentation we could replace the duplicate SMILES from the training set by different synonyms.
    This way, the model would not have these overrepresented SMILES, making it harder to overfit on these molecules.
    The drawback is, however, that the original canonicalized SMILES would be severely underrepresented and the bias towards these overrepresented molecules would still be present.

    With spectral augmentation, instead of randomly augmenting 20\% of the spectra, we could augment the spectra of the underrepresented SMILES to have a more equal distribution of different molecules.
    However, with the severity of the imbalance in the dataset, this could introduce too much augmentation that would drown the original data.

\end{description}


\section{Future for de novo structure prediction}

The performance of the autoregressive models predicting string-based molecular representations is very poor.
These models are not able to predict fully correct molecular structures from \ac{MS/MS}, as the complete structural accuracy on the validation and test set is close to zero percent.
Partly, this can be due to the imperfect training setup discussed in Section \ref{sec:overfitting}, there might be too many biases for the model to overcome.
However, some of these biases can be fixed.
The data imbalance could be mitigated by annotating more \ac{MS/MS} spectra and filtering out overrepresented molecules.
Compared to other molecular datasets, MassSpecGym's labeled dataset is still relatively small.
While additional data is highly desirable, as in most machine learning contexts, obtaining it remains a significant challenge.
There are also a lot of methods that combat the exposure bias, e.g. scheduled sampling \cite{bengio2015scheduled} and professor forcing \cite{lamb2016professor}.
All these methods can not entirely mitigate the exposure bias problem, as this remains an inherent problem when training autoregressive models.

With poor performing models suffering from biases and overfitting, more attention need to be paid to the number of valid and novel predictions, as currently these metrics are seldom mentioned in research for de novo prediction. 
These metrics give a more complete image of a model's generalization capabilities and should be given higher priority in evaluating model performance.

It could be questioned if using autoregressive models and string-based molecular representations is the best method for the de novo structure prediction.
While writing this thesis, a new model, DiffMS \cite{bohde2025diffms}, was published that significantly alters the structure predicting pipeline.
Instead of using an autoregressive decoder to predict molecular structures, DiffMS uses a discrete graph diffusion model to generate the molecular graph itself.
This model did succeed to predict 2.3\% of the test molecules for top-1 evaluation and 4.25\% for top-10 evaluation of MassSpecGym's test set.

Only recently has MassSpecGym introduced a standardized dataset to compare performance. De novo molecular structure prediction from \ac{MS/MS} is a very challenging task,
but even within a few months has DiffMS proven that the string-based autoregressive implementation is flawed.
Using graph-based models to generate molecular structures seems to be the way forward instead of trying to use the methods from the natural language processing domain to predict string-based molecular representations.

\newpage
\section*{Disclaimer, use of generative AI}

For this thesis, code generated by GPT-4o was used to speed up the design of the plots.
For the writing of this Thesis, GPT-4o helped rewriting certain sentences for clarity.