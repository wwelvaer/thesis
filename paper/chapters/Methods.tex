\chapter{Methods and Materials}
\label{chap:methods}

All code used for this thesis can be found on \url{https://github.com/wwelvaer/thesis/tree/main/MassSpecGym}

\section{Model Training}
\label{sec:training}

All models trained for this thesis use the dataset described in section \ref{subsec:massspecgymdataset} and follow the de novo model architecture from MassSpecGym~\cite{bushuiev2024massspecgym} closely.
Most of the model architecture code was heavily inspired by MassSpecGym's source code.
It uses the pytorch transformer implementation along with a linear layer before and after the transformer to achieve the requested input and output dimensions.
The training algorithm follows the original transformer implementation from \textcite{vaswani2017attention}.
The result is a model that predicts the probability distribution of the next token given the peaks of a \ac{MS/MS} spectrum along with an already generated context sequence of tokens.
Spectrum peaks are provided as a 2d array with two columns. Each row corresponds to one peak in the spectrum with the value in the first column being its m/z value and the second column its intensity.
The model acts as a classifier for the next token by outputting a vector of length $n$, with $n$ being the number of tokens of the tokenizer.

The exact model used in MassSpecGym suffered from exploding gradients in the first linear layer, which caused the computation to halt by overflow errors.
Several solutions were implemented to combat this problem.

\begin{description}
    \item[Gradient clipping]limits the gradient to an interval (often [-1, 1]),
    when during backpropagation a gradient is not in the interval,
    it is clipped to the closest boundary.
    \item[Lowering learning rate]of the affected layer delays the gradients from overflowing.
    \item[Scaling] the m/z values from the peaks in the \ac{MS/MS} spectrum down to the same interval as the intensities from the peaks prevents large differences in the gradients calculation.
    \item[Lowering the floating point precision]by shortening the mantissa to 7 bits instead of 23 bits (see figure~\ref{fig:bf16}), 
    all values get rounded to a lower precision. This rounding acts as a regularization step.
    \begin{figure}[h]
        \centering
        \includegraphics[width=\linewidth]{figures/methods/bf16.JPG}
        \caption{Composition of float32 and bfloat16 (from \url{https://en.wikipedia.org/wiki/Bfloat16_floating-point_format})}
        \label{fig:bf16}
    \end{figure}
    \item[Removing embedding scaling,]because the order of the peaks from the input spectra does not matter,
    no positional encodings are added, there is therefor also no need to scale the embedding layer as in the original implementation from \textcite{vaswani2017attention}.
\end{description}

Removing the embedding scaling was the only method that fully stabilized the training without tanking the performance.
All models described further therefore share the same model architecture as the MassSpecGym de novo models without the embedding scaling step.

Every model was trained with an early stopping criterion of five epochs of non-improving validation loss.

All models were trained on the Ugent \ac{HPC} accelgor cluster using a singular NVIDIA Ampere A100 GPU.
Training time without sampling was for every model around 2 hours. 

% Verplaatsen naar results?
To replicate the results from MassSpecGym, a hyperparameter gridsearch was conducted taking the optimal hyperparameters (= hyperparameters from the model with the lowest validation loss) from the MassSpecGym results into account
while also extending the search space further than the previously found optimal values.

%%

\section{Samplers}
\label{sec:samplers}

The MassSpecGym paper~\cite{bushuiev2024massspecgym} implemented a simple naive sampling algorithm that iteratively samples from the (with temperature scaled) output distribution of the model, described as the naive approach in section \ref{sec:samplingmethods}.
This is repeated $n$ times, with $n$ being the prediction amount for each spectrum. The randomness of the sampler results (optimally) in $n$ different predictions.
Because we are interested in a relaxation of the prediction problem, where $n=10$ predictions are evaluated, the sample algorithm will be executed 10 times. This can be optimized with parallelization.

A big load of the sampling calculation is the forward pass of the transformer model. 
If we work with a GPU that has enough VRAM (at least $n$ times more than would be used for sampling one batch),
we can duplicate the batch $n$ times and feed it trough one forward pass of the model. The inherent parallelization of the GPU will then be used more optimally.
An execution time benchmark can be found in section \ref{sec:sampler_parallelization} of the appendix.

To benchmark performance of the different sampling algorithms described in section \ref{sec:samplingmethods}, an implementation was made for each algorithm.
Because the VRAM requirement for the parallelization can be very demanding, two versions of each algorithm were implemented. 
One version uses the parallelization by reducing the number of forward passes (requiring sufficient VRAM on the GPU), the other version repeats the sampling of one batch just like the original MassSpecGym implementation.

\subsection{Stochastic samplers}

Stochastic autoregressive sampling algorithms introduce some randomness by sampling from the output distribution is some way.
This allows them to be called multiple times to get different predictions from the same spectra as described before.
The simplest stochastic sampling algorithm is the naive method described above.
The logits from the output of the transformer model are first scaled by a temperature parameter to dictate how random the predictions should be.
A very low temperature will force the model to predict only the token with the highest logit while a very high temperature will force the model to pick a token at random.
After the logits are scaled, a softmax function maps these to probabilities. These probabilities are sampled by the multinomial function from pytorch.
This is repeated until the maximum prediction length is reached or every prediction in the batch reached the end token.

To reduce randomness and prevent that tokens with small probabilities can be sampled, we can limit the sampling to the top-$k$ tokens.
Because the tokens with the highest logits will also map to the highest probabilities through the softmax function,
we can isolate the $k$ tokens with the highest logits before applying the softmax function.
The output probabilities are then calculated for only these $k$ isolated tokens by applying the softmax function to only these logits, which are then sampled with the multinomial function.

Another way to reduce randomness and prevent tokens with low probabilities to be sampled without explicitly declaring the number of tokens $k$ that should be considered is by adding a cumulative probability threshold $p$ with top-p/nucleus sampling.
The logits are first passed trough the softmax function to get their probabilities.
The tokens are then sorted by their probabilities in a descending order.
From these sorted probabilities, a cumulative distribution is calculated.
The first $m$ tokens are then selected for which their cumulative probability just exceeds the threshold $p$.
This results in the smallest subset of tokens with a cumulative probability exceeding $p$.
These $m$ tokens are then isolated and their probabilities rescaled through the softmax function before being sampled with the multinomial function.

\subsection{Deterministic samplers}

Because there is no randomness in deterministic samplers, the sampling algorithm is not repeated to get $n$ different predictions. The parallelization optimization is hence not applicable here and only one version of these samplers are implemented.

A greedy approach to remove randomness is to always sample the token with the highest probability. Because this is the same as the token with the highest logit, no softmax function is needed.
Again, this is repeated until the maximum prediction length is reached or every prediction in the batch reached the end token.
This greedy sampler will be only capable of generating one prediction for a spectrum.
Other samplers can become identical to the greedy sampler by choice of parameters, e.g. top-$k$ sampler with $k=1$, beam-search with only one beam \ldots

To generate multiple predictions for the same spectrum in a deterministic way, beam search can be used.
It uses a scoring function to score (partial) prediction sequences and stores the highest-scoring sequences at each sampling time step.
With $b$ being the number of predictions stored, called beam width.
The beam width decides the size of the search space in the algorithm, as only the highest scoring prediction sequences from the previous sampling time step will be expanded.
It is a greedy breadth-first search algorithm that maximizes a scoring function.

The used scoring function that scores a sequence of tokens $y$ with length $L$ is:
\[score(y) = \frac{1}{L^a} \sum\limits_{t=1}^{L}log\ P(y_t | y_{t-1},\dots,y_{1})\]
It returns the product of the probabilities of token sequences (to make it more computational feasible, the sum of log probabilities).
Because the scoring function will be used to score sequences with different lengths (as certain sequences will reach the end token faster than others),
we have to scale the score by its prediction length. Otherwise shorter sequences will be scored higher.
Parameter $a$ allows the scoring function to be tuned for certain prediction lengths.
If we desire shorter predictions, $a$ will be closer to 0.
If we want to score sequences ignoring their length, $a=1$. 
If longer sequences are preferred, $a>1$.

At each time step the beam search algorithm uses the transformer model to predict the next token probabilities for each sequence stored in the previous time step.
From these probabilities and their sequence context, new scores are calculated.
The sequences (context + new token) with the highest $b$ scores are then stored for expansion in the next time step.
This is repeated until the maximum prediction length is reached or every $b$ sequences for every spectrum in the batch has reached the end token.

By using a beam width $b=1$, the beam search algorithm can be used as a greedy sampler.

\section{BPE as Pretraining}
\label{sec:bpe}

A tokenizer maps a token to a (sequence of) character(s) and back. The set of (sequence of) characters in a tokenizer is called its vocabulary.
All tokenizers in this Thesis for which it's vocabulary was computed with \acf{BPE}, used the ByteLevelBPETokenizer implementation from huggingface's tokenizers library (\url{https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/byte_level_bpe.py}).
The \ac{BPE} algorithm combines a frequently occurring pair of characters by replacing them with a novel character and storing this replacement in a lookup table.
This novel character can then also be combined further with other characters. 
This is repeated until either the lookup table has reached the desired vocabulary size or there are no more frequently occurring pair of characters in the modified dataset.
These replacements from the lookup table can then be reconstructed to their original sequence of characters.
These reconstructed sequences denote frequently occurring sequences in the dataset and are stored in the vocabulary of the tokenizer.
Because of the replacements in the dataset, these sequences can very in length.
Two important parameters can be set in this algorithm:
(1) The vocab size discussed before. This dictates the maximum number of sequences that can be stored in the vocabulary and will thus decide if the algorithm will be halted early. The default value is 30,000.
(2) The minimum frequency a pair of characters has to occur in the dataset to be stored. The default value is 2.

The unlabeled molecule datasets used to compute the vocabulary from the different tokenizers in this Thesis are from MassSpecGym, see section \ref{subsec:massspecgymdataset}.
Each \ac{BPE} tokenizer also used the unique training labels from the training set.
The SMILES de novo model from the MassSpecGym paper \cite{bushuiev2024massspecgym} used the dataset with 4,000,000 unlabeled SMILES to precompute a tokenizer vocabulary using \ac{BPE}.
The SELFIES de novo model from the paper did not use \ac{BPE}, the vocabulary of its tokenizer only contains sequences of a single SELFIES tokens.

\section{Augmentation}
\label{sec:augmentation}

All datasets were augmented offline, before training, to preserve execution time on the \ac{HPC} of Ghent University.
Code for the data augmentation can be found as a notebook at \url{https://github.com/wwelvaer/thesis/blob/main/notebooks/augmentation.ipynb}.
For all augmented training sets, a SMILES transformer model was trained with the same optimal hyperparameters from the MassSpecGym paper \cite{bushuiev2024massspecgym}.

\subsection{SMILES augmentation}

SMILES are a one-dimensional representation of a 2d molecular graph.
The traversal algorithm to convert a molecular graph to a SMILES is deterministic.
The same molecular graph will always be converted to the same SMILES.
We can randomize this traversal algorithm to create different SMILES for the same molecular graph.
The SMILES are then synonyms of each other, as they represent the same molecule.
RDkit has already implemented the randomized traversal in their MolToSmiles function (\url{https://www.rdkit.org/docs/source/rdkit.Chem.rdmolfiles.html#rdkit.Chem.rdmolfiles.MolToSmiles}).

To generate a SMILES synonym, the original SMILES must be converted to its molecular graph. 
By using the randomized traversal from that graph back to a SMILES we get a synonym.
We can now augment the training dataset by copying the spectrum of a training sample but changing the SMILES label to a random synonym.

Three different augmented training sets were created. One where the training data is duplicated once, twice and five times with SMILES synonyms.
The original training samples with the original SMILES are kept in the datasets.

\subsection{Spectral augmentation}

The \ac{MS/MS} spectra of two identical molecules can be shifted because of different precursor masses.
We can mimic this to augment the training spectra.
Following the methods of DreaMS \cite{bushuiev2024emergence}, all augmented spectra are randomly shifted by a maximum m/z range of 50.
The SMILES labels from the augmented spectra are copied.

Two different augmented training sets were created. One mimicking DreaMS where $20\%$ of the training spectra are augmented, and one where all training spectra are augmented.
Again, the original training data is kept in the augmented training sets.

\section{Molecular representations}
\label{sec:representations}

All molecular representations used for training different models were translated from the SMILES in the MassSpecGym dataset.
For translation, RDkit was used except for DeepSMILES and SELFIES, these molecular representations have their own packages with the same name.
To ease the implementation of models predicting different molecular representations than SMILES,
we can add a translation step before the model input and after the model output.
This allows the model to receive SMILES for training and output SMILES for evaluating,
while training on, and predicting another molecular representation.
The translation occurs thus at training and evaluation time.
This method is also used for the de novo SELFIES model in the MassSpecGym paper (see section \ref{subsec:massspecgymmodels}).
The tokenizer is used to handle translation. Before every encoding, and after every decoding step, a translation step from and to SMILES is performed.
The code used for all tokenizers can be found at \url{https://github.com/wwelvaer/thesis/blob/main/MassSpecGym/mol_tokenizers.py}.

Because SMILES can be rewritten during translation to and from another molecular representation.
The training SMILES are passed through each representation's tokenizer to compute the actual training labels for the corresponding models.
This way the amount of de novo predictions can be accurately measured with different molecular representations.

Multiple models were trained on the following molecular representations: SMILES, DeepSMILES, SELFIES and InchI.
For each of the four representations a model was trained using a tokenizer pre-computed with \ac{BPE} on the unlabeled 4 million molecules dataset,
mimicking the implementation from the de novo SMILES model from MassSpecGym. All models used the hyperparameters from this model.
For SMILES, DeepSMILES and SELFIES, a model was also trained using a tokenizer without \ac{BPE}.

To respect the more complex composition of InchI, a model with multiple decoders was created to predict the different layers of an InchI string.
Because only the first three layers encode the 2d composition of a molecule, and we are not interested in 3d composition information such as stereochemistry,
the model only predicts these three layers.
Three decoders are used to predict these layers. These decoders share the exact same architecture used by the decoders from the other models.
Each decoder has its own tokenizer. For simplicity, a parent tokenizer handles these three tokenizers and handles the correct merging of the three predictions by the model.
To train the model, the loss of the model is the sum of the loss from the three decoders.
